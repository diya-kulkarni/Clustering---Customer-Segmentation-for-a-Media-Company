# -*- coding: utf-8 -*-
"""Assignment3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZJqaGebRadq70a0JlWHzh-LMU28Uf41N

# Assignment 3
Assignments are an individual assessment, you should not work in groups.

You will be turning in:

1. A README.md with all the relevant information. Please see example on Canvas under the Course Docs module.
2. An .ipynb with just your code (show all code necessary for the analysis, but remove superfluous code)
3. A PDF with your Report (see [Google Docs Template](https://docs.google.com/document/d/1idQDg0U4X4NCEfdLy3YV72njgt8KOmuv7ujv1soxiF4/edit?usp=sharing))


## Data

We're going to do some customer segmentation for a **media company** (a digital magazine) to help them understand the needs of their customers better, and put them into useful groups of similar clusters. There are two datasets each of which is a random sample of `200` customers.

### Behavioral Data

[This dataset](https://github.com/katherinehansen2/CPSC392Hansen/blob/main/data/HW3_behavioral.csv) contains information about the media company customers' behavior on the site. The variables in the customer data include:

- `id`: customer id
- `gender`: self-disclosed gender identity, `male`, `female`, `nonbinary` or `other`
- `age`: age in years
- `current_income`: self-reported current annual income in thousands
- `time_spent_browsing`: average number of minutes spent browsing website per month
- `prop_ad_clicks`: proportion of website ads that they click on (between `0` and `1`)
- `longest_read_time`: longest time spent consecutively on website in minutes
- `length_of_subscription`: number of days subscribed to the magazine
- `monthly_visits`: average number of visits to the site per month



### Article Data

[This dataset](https://github.com/katherinehansen2/CPSC392Hansen/blob/main/data/HW3_topics.csv) contains information about the number of articles customers read in each topic in the past 3 months. The topics in the customer data include:

- `Stocks`
- `Productivity`
- `Fashion`
- `Celebrity`
- `Cryptocurrency`
- `Science`
- `Technology`
- `SelfHelp`
- `Fitness`
- `AI`

## 1. Behavioral Clustering
- Make ggplot scatterplots of pairs of your features to give you a little bit of information about the data, and to help you decide which algorithm to use (you donâ€™t need to make scatterplots for all possible pairs of features, just make sure each feature appears at least once).
- Using `sklearn` build and fit **one** clustering model (choose from **K-Means**, **Gaussian Mixture Models**, **DBSCAN**, and **Hierarchical Clustering**) using all the continuous/interval variables *except* `id`.
- Appropriately Z-score continuous/interval variables.
- Choose the number of clusters (if applicable) and in the **Methods** section of your report, explain how you chose. Also explain **why/how** you chose any relevant hyperparameters (such as linkage, distance metric, eps, min samples...).
- In the **Methods** section of your report, explain **in detail** both the *pros* and *cons* of the all the clustering models (e.g. what type of data do they work well for? Are there any possible downsides of using them?) and *provide a justification* for why you chose the clustering algorithm that you did.
- Using **plotnine/ggplot** and **PCA**, create a scatterplot of your clusters by plotting the first Principal Component (PC1) on the x-axis, and the second Principal Component (PC2) on the y-axis. Color the points by their cluster. Include this plot in your **Results** Section. Plots made with other libraries will recieve a score of 0 for this section.
- (DO NOT cluster on the PCs, you're only using the PCs to visualize the clusters you fit on the original features)
- Create at least one *other* summary of what kind of customers are in each cluster. This could be a different ggplot (built using plotnine), or a neatly formatted summary table (hint: `groupby()`). In the **Results** section, discuss in detail:
    - what kind of customers are in each cluster
    - how the clusters "performed" (e.g. are they good clusters? How can you tell?)
    - how that information might help the company

## 2. Article Clustering
- Using `sklearn` build and fit a **Hierarchical Clustering** model using all the variables *except* `id`. Use cosine similarity as your distance metric (also called `affinity`) and `average` linkage.
- Do **not** z-score, as these are counts.
- Create a dendrogram and use it to determine the number of clusters you'll use. In the **Methods** section of your report, explain **what number** of cluster you decided to use and **why**.
- In the **Results** section, discuss the performance of the model based on the dendrogram.
- Create at least one other summary of what kind of customers are in each cluster. This could be a different ggplot (built using plotnine), or a neatly formatted summary table (hint: `groupby()`). In the **Results** section, discuss in detail what kind of customers are in each cluster, and how that information might help the company.

## 3. Report

[DOCS TEMPLATE HERE](https://docs.google.com/document/d/1idQDg0U4X4NCEfdLy3YV72njgt8KOmuv7ujv1soxiF4/edit?usp=sharing)

Your Technical Report is a way to practice presenting and formatting your results like you would in industry. Make sure your report and **plots** are clear, and explain things clearly. Write a report that has the following sections:

1. **Introduction**: description of the problem (e.g. what are you predicting? what variables do you have available? How might this model be useful if you are successful). You should end with a sentence or two about what the impact of these models could be.

2. **Methods**: describe your models in detail (as if explaining them to the CEO of the media company), as well as any pre-processing you had to do to the data.

3. **Results**: How did your models perform? Describe the clusters from your Behavioral Clustering models. Were they different? If so, how? What could the CEO of the media company do with these clusters? Describe the clusters form your Article Clustering model. What could the CEO of the media company do with these clusters?

4. **Discussion/Reflection**: A few sentences about what you learned from performing these analyses, and at least one suggestion for what you'd add or do differently if you were to perform this analysis again in the future.
"""

# Commented out IPython magic to ensure Python compatibility.
#imports
import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
from plotnine import *

from sklearn.preprocessing import StandardScaler #Z-score variables

from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture

from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
from sklearn.cluster import AgglomerativeClustering

from itertools import combinations

import matplotlib.pyplot as plt
import seaborn as sns

import scipy.cluster.hierarchy as sch

# %matplotlib inline

"""**BEHAVIORAL CLUSTERING**"""

#load data with only continuous/interval variables

behavioral = pd.read_csv("https://raw.githubusercontent.com/katherinehansen2/CPSC392Hansen/refs/heads/main/data/HW3_behavioral.csv")
behavioral.head()

#check for missing values
behavioral.isnull().sum() #no missing values; no dropping values

#predictors & standardizing
predictors = ["age", "current_income", "time_spent_browsing", "prop_ads_clicked", "longest_read_time", "length_of_subscription", "monthly_visits"]

X = behavioral[predictors]

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

#create empty model
gmm = GaussianMixture(n_components=4)

#fit model + predict
labels = gmm.fit_predict(X_scaled)

#add cluster labels to the original DataFrame
behavioral["clusters"] = labels

#scatterplots of predictors; for each combination of predictors
combos = list(combinations(predictors, 2)) #generate all pairs of combinations

# Plot each pair of predictors
plots = []
for x, y in combos:
    plot = (
        ggplot(behavioral, aes(x=x, y=y, color="factor(clusters)")) +
        geom_point() +
        theme_minimal() +
        labs(
            x=x.replace("_", " ").title(),
            y=y.replace("_", " ").title(),
            title=f"{x.title()} vs {y.title()}",
            color="Clusters"
        )
    )
    plots.append(plot)

for plot in plots:  #display the plots
    display(plot)

#use BIC to determine number of clusters to be used

# create dictionary
metrics = {"bic": [], "k": []}
#test ranges in GMM
for i in range(1,10):
    gmm = GaussianMixture(i)
    labels = gmm.fit_predict(X_scaled)
    bic_score = gmm.bic(X_scaled)

    metrics["bic"].append(bic_score)
    metrics["k"].append(i)

df = pd.DataFrame(metrics)

(ggplot(df, aes(x = "k", y = "bic")) +
  geom_line() + theme_minimal() +
    labs(x = "K", y = "BIC Score",
         title = "BIC Scores for Different Ks"))

#create empty model - GMM
gmm = GaussianMixture(n_components=4)

#fit model + predict
labels = gmm.fit_predict(X_scaled)

#add cluster labels to the original DataFrame
X["clusters"] = labels

#plot results
(
    ggplot(X, aes(x="current_income", y="length_of_subscription", color="factor(clusters)")) +
    geom_point() +
    theme_minimal() +
    labs(
        x="Current Income",
        y="Length of Subscription",
        color="Clusters"
    )
)

#pca
pca = PCA()
pca.fit(X)

#scree plot
pcaDF = pd.DataFrame({"expl_var" :
                      pca.explained_variance_ratio_,
                      "pc": range(1,len(pca.explained_variance_ratio_)+1),
                      "cum_var":
                      pca.explained_variance_ratio_.cumsum()})
#display
(ggplot(pcaDF, aes(x = "pc", y = "expl_var")) + geom_line() + geom_point() +
 theme_minimal() + labs(x = "PCs", y = "Proportion of Explained Variance", title = "Scree Plot"))

#cumulative variance
pcaDF2 = pd.concat([pcaDF, pd.DataFrame({"pc": [0], "cum_var": [0]})], ignore_index=True) # add 0,0 for reference

(ggplot(pcaDF2, aes(x = "pc", y = "cum_var")) + geom_line() +
 geom_point() + geom_hline(yintercept = 0.95, color = "red", linetype = "dashed") +
 theme_minimal() + labs(x = "Number of PCs", y = "Cumulative Proportion of Explained Variance", title = "Cumulative Variance Plot"))

#summary of what kind of customers are in each cluster (using summary table)
cluster_summary = behavioral.groupby('clusters')[predictors].agg(['mean','std','median','min','max'])
cluster_summary

"""**ARTICLE CLUSTERING**"""

#load data

article = pd.read_csv("https://raw.githubusercontent.com/katherinehansen2/CPSC392Hansen/refs/heads/main/data/HW3_topics.csv")
article.head()

#check for missing values

article.isnull().sum() #no missing values; no dropping values

#set up X/features

features = ["Stocks", "Productivity", "Fashion", "Celebrity", "Cryptocurrency", "Science", "Technology", "SelfHelp", "Fitness", "AI"] #no id
X = article[features]

#hac
hac = AgglomerativeClustering(linkage = "average",
                              metric = "cosine",
                              distance_threshold=0,
                              n_clusters = None)

#fit and get labels
labels = hac.fit_predict(X[features])

#plot dendrogram to figure out number of clusters

#function for dendrogram from sklearn
# from sklearn: https://github.com/scikit-learn/scikit-learn/blob/70cf4a676caa2d2dad2e3f6e4478d64bcb0506f7/examples/cluster/plot_hierarchical_clustering_dendrogram.py
def plot_dendrogram(hac, **kwargs):

    # create the counts of samples under each node
    counts = np.zeros(hac.children_.shape[0])
    n_samples = len(hac.labels_)
    for i, merge in enumerate(hac.children_):
        current_count = 0
        for child_idx in merge:
            if child_idx < n_samples:
                current_count += 1  # leaf node
            else:
                current_count += counts[child_idx - n_samples]
        counts[i] = current_count

    linkage_matrix = np.column_stack(
        [hac.children_, hac.distances_, counts]
    ).astype(float)
    # Plot the corresponding dendrogram
    sch.dendrogram(linkage_matrix, **kwargs)

#call function using article hac
plot_dendrogram(hac, color_threshold = 5)

#refit hac with specific number of clusters
hac = AgglomerativeClustering(linkage = "average",
                              metric = "cosine",
                              n_clusters = 3)

#fit and get labels
labels = hac.fit_predict(X[features])

#silhouette score
print(silhouette_score(X_scaled, labels))

#look at cluster performance
article["cluster_3"] = labels
gg_list = []
for test in features:
    title = "Test " + test.capitalize() + " Cluster Performance"
    gg_list.append(ggplot(article, aes(x = "factor(cluster_3)", y = test))
          + geom_boxplot(aes(fill = "factor(cluster_3)")) +
          theme_minimal() +
          scale_fill_discrete(name = "Cluster Assignment") +
          labs(x = "Cluster",
               y = "Score",
               title = title))

gg_list[0]

gg_list[1]

gg_list[2]

gg_list[3]

gg_list[4]

gg_list[5]

gg_list[6]

gg_list[7]

gg_list[8]

gg_list[9]

#summary table

cluster_summary = article.groupby('cluster_3')[features].agg(['mean','std'])
cluster_summary